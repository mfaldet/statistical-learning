---
title: "Ch4Homework"
author: "Mac Faldet"
date: "March 6, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 4
# 4.7 Exercises

## 1. Logistic Function
Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). 
In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.

```
Prove:
p(X)    =   eβ0+β1X
           -----------
           1 + eβ0+β1X


From:
  p(X)  =  eβ0+β1X
--------
1 − p(X) 

p(X)    = (1 − p(X))eβ0+β1X
  
  
Thus:
p(X)    =   eβ0+β1X
          -----------
          1 + eβ0+β1X
```


## 3.
This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. 
We consider the simple case where p = 1; i.e. there is only one feature.
Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, X ∼ N(μk, σ2k). 

Recall that the density function for the one-dimensional normal distribution is given in (4.11). 
Prove that in this case, the Bayes’ classifier is not linear. Argue that it is in fact quadratic.

```
p_k(x)      = pi_k*{1/(sqrt{2*pi}*sigma_k)} *
                exp(-1/{2*sigma_k^2} (x-mu_k)^2)
                 ----------------------------------
               sum 
                  {pi_k*{1/(sqrt{2*pi}*sigma_k)} *
                exp(-1/{2*sigma_k^2} (x-mu_k)^2)}

         
log(p_k(x)) = log(pi_k) + log(1/(sqrt{2*pi}) * sigma_k) 
              * -1/{2*sigma_k^2} (x-mu_k)^2
              ------------------------------------------
              log(sum 
                    {pi_l*(1/{sqrt{2*pi}) * sigma_l} 
                    * exp(-{1/{2*sigma_l^2}}(x-mu_l)^2)})
         
         
            = log(pi_k)  +  log(1/{sqrt{2*pi}*sigma_k})  +  -1/{2*sigma_k^2}(x - mu_k)^2
  

delta(x) = log(pi_k)  +  log(1/{sqrt{2*pi}*sigma_k})  +   -1/{2*sigma_k^2}(x - mu_k)^2
```

You may consider delta(x) a quadratic function, given the ability to summarize larger operations into unknown variables and observing it's quadratic form.

```
log(pi_k) = A
log(1/{sqrt{2*pi}*sigma_k}) = B 
-1/{2*sigma_k^2}(x - mu_k) = C

A + BX + Cx^2
```

## 4.
When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. 
This phenomenon is known as the curse of dimensionality, and it ties into the fact that curse of dinon-parametric approaches often perform poorly when p is large. We mensionality will now investigate this curse.

### 4.a
Suppose that we have a set of observations, each with measurements on p = 1 feature, X. 
We assume that X is uniformly (evenly) distributed on [0, 1]. 
Associated with each observation is a response value. 
Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of X closest to that test observation. 
For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65].
On average, what fraction of the available observations will we use to make the prediction?

A: Roughly 10%


### 4.b
Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. 
We assume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. 
We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. 
For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. 
On average, what fraction of the available observations will we use to make the prediction?


A: Roguhly 1%


### 4.c
Now suppose that we have a set of observations on p = 100 features. 
Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. 
We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation.
What fraction of the available observations will we use to make the prediction?

A: Roughly 10^(-98) 


### 4.d
Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.

A: As p increases observations that are  still "near" decrease exponentially.


### 4.e
Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. 
For p = 1, 2, and 100, what is the length of each side of the hypercube? 
Comment on your answer.

A: take 10% (or .1) to the power of p. Thus,
p=1 ~ .1
p=2 ~ .32
p=3 ~ .46
p=100 ~ .1^(.01)




## 6.
Suppose we collect data for a group of students in a statistics class with variables
X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. 
We fit a logistic regression and produce estimated coefficient, `βˆ0 = −6, βˆ1 = 0.05, βˆ2 = 1`.

### 6.a
Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

```
   exp(-6+.05X1+X2)
=  ------------------
   1+exp(-6+.05X1+X2)

   exp(-6+.054+3.5)
=  ------------------
   1+exp(-6+.054+3.5)
   
   exp(-.5)
=  ----------
   1+exp(-.5)

= 37.75%
```

A: 37.75%


### 6.b
How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?

```
   exp(-6+.05X1+X2)
=  ------------------
   1+exp(-6+.05X1+X2)

   exp(-6+.05X+3.5)
=  ------------------
   1+exp(-6+.05X+3.5)
   
.5(1+exp(-2.5+.5X)) = exp(-2.5+0.5X)
.5+.5exp(-2.5+.05X)) = exp(-2.5+.05X)
.5 = .5exp(-2.5+.05X)
1 = -2.5+.05X
X = 2.5/.05
  = 50
```
A: 50hours



## 9.
This problem has to do with odds.

### 9.a
On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

.37/1.37 = .27

A: 27%


### 9.b
Suppose that an individual has a 16 % chance of defaulting on her credit card payment. 
What are the odds that she will default?

.16/.84 = .19

A: 19%




## 13.
Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. 
Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.

```{r}
library(MASS)
summary(Boston)
```

```{r}
attach(Boston)
crime01 = rep(0, length(crim))
crime01[crim > median(crim)] = 1
Boston = data.frame(Boston, crime01)

train = 1:(dim(Boston)[1]/2)
test = (dim(Boston)[1]/2 + 1):dim(Boston)[1]
Boston.train = Boston[train, ]
Boston.test = Boston[test, ]
crime01.test = crime01[test]
```

```{r}
# logistic regression
glm.fit = glm(crime01 ~ . - crime01 - crim, data = Boston, family = binomial, 
    subset = train)
```

```{r}
glm.fit = glm(crime01 ~ . - crime01 - crim - chas - tax, data = Boston, family = binomial, subset = train)
```

```{r }
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != crime01.test)
```

```{r}
# LDA
lda.fit = lda(crime01 ~ . - crime01 - crim, data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
mean(lda.pred$class != crime01.test)
```

```{r}
# KNN
library(class)
train.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, 
    lstat, medv)[train, ]
test.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, 
    lstat, medv)[test, ]
train.crime01 = crime01[train]
set.seed(1)
# KNN(k=1)
knn.pred = knn(train.X, test.X, train.crime01, k = 1)
mean(knn.pred != crime01.test)
```

```{r}
# KNN(k=10)
knn.pred = knn(train.X, test.X, train.crime01, k = 10)
mean(knn.pred != crime01.test)

# KNN(k=100)
knn.pred = knn(train.X, test.X, train.crime01, k = 100)
mean(knn.pred != crime01.test)
```

